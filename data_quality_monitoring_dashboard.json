{
	"jobConfig": {
		"name": "data_quality_monitoring_dashboard",
		"description": "",
		"role": "arn:aws:iam::633763687748:role/Glurrunrole",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "data_quality_monitoring_dashboard.py",
		"scriptLocation": "s3://aws-glue-assets-633763687748-ap-southeast-2/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-11-28T17:27:25.110Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-633763687748-ap-southeast-2/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-633763687748-ap-southeast-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nimport json\r\nfrom datetime import datetime\r\nfrom pyspark.context import SparkContext\r\nfrom pyspark.sql import SQLContext\r\nfrom pyspark.sql.functions import col, when, count\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.utils import getResolvedOptions\r\nimport boto3\r\n\r\n# Step 1: Initialize Glue Context and Spark Session\r\nsc = SparkContext.getOrCreate()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\nsqlContext = SQLContext(sc)\r\n\r\n# Step 2: Read arguments passed from Lambda via Glue\r\nargs = getResolvedOptions(sys.argv, [\r\n    'BUCKET_NAME',\r\n    'OBJECT_KEY',\r\n    'SNS_TOPIC_ARN',\r\n    'ALERT_THRESHOLD',\r\n    'EXPECTED_SCHEMA'\r\n])\r\n\r\n# Extract parameters\r\nbucket_name = args['BUCKET_NAME']\r\nobject_key = args['OBJECT_KEY']\r\nsns_topic_arn = args['SNS_TOPIC_ARN']\r\nalert_threshold = float(args['ALERT_THRESHOLD'])\r\nexpected_schema = json.loads(args['EXPECTED_SCHEMA'])\r\n\r\n# Derive paths\r\ns3_input_path = f\"s3://{bucket_name}/{object_key}\"\r\ns3_output_path = f\"s3://{bucket_name}/output\"\r\n#conflict_output_folder = f\"s3://{bucket_name}/conflict_records\"\r\n\r\nprint(f\"Processing file: {s3_input_path}\")\r\nprint(f\"Saving results to: {s3_output_path}\")\r\n\r\n# Step 3: Read input data from S3\r\ninput_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(s3_input_path)\r\nprint(\"Data Schema: \")\r\ninput_df.printSchema()\r\n\"\"\"\r\n# Step 4: Detect and handle data type conflicts\r\ndef detect_conflicts_with_inferred_schema(df, schema):\r\n    conflict_conditions = []\r\n    \r\n    # Function to handle invalid numeric data\r\n    def handle_invalid_numeric(df, column_name, expected_type):\r\n        if expected_type == \"double\":\r\n            # Check if the column can be cast to double and ensure the value is numeric (not a string)\r\n            return ~col(column_name).cast(\"double\").isNotNull() & col(column_name).rlike(\"^[0-9]*\\\\.?[0-9]+$\")\r\n        elif expected_type in {\"integer\", \"long\"}:\r\n            # Check if the column can be cast to bigint and ensure the value is numeric (not a string)\r\n            return ~col(column_name).cast(\"bigint\").isNotNull() & col(column_name).rlike(\"^[0-9]+$\")\r\n        return None\r\n\r\n    # Loop through each column and check for data type conflicts\r\n    for column_name, expected_type in schema.items():\r\n        if column_name not in df.columns:\r\n            print(f\"Column {column_name} not found in DataFrame. Skipping...\")\r\n            continue\r\n\r\n        if expected_type == \"double\":\r\n            # Cast to double and check for non-numeric values\r\n            conflict_conditions.append(~col(column_name).cast(\"double\").isNotNull() & col(column_name).isNotNull())\r\n        elif expected_type in {\"integer\", \"long\"}:\r\n            # Cast to integer or long and check for non-numeric values\r\n            conflict_conditions.append(~col(column_name).cast(\"bigint\").isNotNull() & col(column_name).isNotNull())\r\n        else:\r\n            # Handle invalid strings for numeric columns\r\n            conflict_condition = handle_invalid_numeric(df, column_name, expected_type)\r\n            if conflict_condition:\r\n                conflict_conditions.append(conflict_condition)\r\n            else:\r\n                #print(f\"Unsupported data type for column {column_name}: {expected_type}\")\r\n                pass\r\n    \r\n    # If no conflict conditions are defined, return an empty DataFrame\r\n    if not conflict_conditions:\r\n        #print(\"No valid conflict conditions found.\")\r\n        return df.limit(0)\r\n    \r\n    # Combine all conflict conditions\r\n    conflict_filter = conflict_conditions[0]\r\n    for condition in conflict_conditions[1:]:\r\n        conflict_filter |= condition\r\n    \r\n    # Apply the filter to detect conflicts\r\n    conflict_df = df.filter(conflict_filter)\r\n    return conflict_df\r\n\r\n# Example: Detect conflicts based on inferred schema\r\ninferred_schema = {field.name: field.dataType.typeName() for field in input_df.schema.fields}\r\nconflict_df = detect_conflicts_with_inferred_schema(input_df, inferred_schema)\r\n\r\n# Save conflicting rows to a separate file\r\nif not conflict_df.rdd.isEmpty():\r\n    current_date = datetime.now().strftime(\"%Y-%m-%d\")\r\n    conflict_output_path = f\"{conflict_output_folder}/wrong_records_{current_date}.csv\"\r\n    conflict_df.write.option(\"header\", True).csv(conflict_output_path)\r\n    print(f\"Conflicting records saved to: {conflict_output_path}\")\r\nelse:\r\n    #print(\"No conflicting records found.\")\r\n    pass\r\n\r\n# Remove conflicting rows from input DataFrame\r\nclean_df = input_df.subtract(conflict_df)\r\n\"\"\"\r\n# Step 5: Data Quality Checks (unchanged)\r\ndef check_missing_values(df):\r\n    total_records = df.count()\r\n    missing_counts = df.select(\r\n        [(count(when(col(c).isNull() | (col(c) == \"\"), c)).alias(c)) for c in df.columns]\r\n    ).collect()[0].asDict()\r\n    print(\"Missing Counts: \", missing_counts)\r\n    missing_percentages = {\r\n        col: (missing_counts[col] / total_records) * 100 for col in missing_counts\r\n    }\r\n    print(\"Missing percentage: \", missing_percentages)\r\n    return missing_percentages\r\n\r\ndef check_duplicates(df, key_columns=None):\r\n    if key_columns is None:\r\n        key_columns = df.columns\r\n    total_records = df.count()\r\n    grouped_df = df.groupBy(key_columns).count()\r\n    duplicate_records = grouped_df.filter(col(\"count\") > 1).count()\r\n    print(\"Duplicate records: \", duplicate_records)\r\n    duplicate_percentage = (duplicate_records / total_records) * 100\r\n    print(\"Duplicate percentage: \", duplicate_percentage)\r\n    return duplicate_percentage\r\n\r\ndef check_data_types(df, schema):\r\n    mismatched_columns = {}\r\n    for column, expected_type in schema.items():\r\n        if column in df.columns:\r\n            actual_type = df.schema[column].dataType.typeName()\r\n            if actual_type != expected_type:\r\n                mismatched_columns[column] = {\r\n                    \"expected\": expected_type,\r\n                    \"actual\": actual_type\r\n                }\r\n    print(\"Mismatched columns: \", mismatched_columns)            \r\n    return mismatched_columns\r\n\r\n# Perform checks\r\nmissing_values_report = check_missing_values(input_df)\r\nduplicate_percentage = check_duplicates(input_df, key_columns=[\"Product ID\"])\r\ndata_type_issues = check_data_types(input_df, expected_schema)\r\n\r\n# Step 6: Generate the report\r\ncurrent_date = datetime.now().strftime(\"%Y-%m-%d\")\r\ndata_quality_report = []\r\n\r\n# Add the current date as an object\r\ndata_quality_report.append({\r\n    \"Date\": current_date\r\n})\r\n\r\n# Missing Values Check\r\nmissing_values_passed = 100 - sum(missing_values_report[col] for col in missing_values_report)  # Calculating passed percentage\r\nmissing_values_failed = sum(missing_values_report[col] for col in missing_values_report)\r\nmissing_values_threshold_met = missing_values_failed <= alert_threshold\r\n\r\ndata_quality_report.append({\r\n    \"check_name\": \"Missing Values\",\r\n    \"passed\": missing_values_passed,\r\n    \"failed\": missing_values_failed,\r\n    \"threshold_percentage\": alert_threshold,\r\n    \"threshold_met\": missing_values_threshold_met\r\n})\r\n\r\n# Duplicates Check\r\nduplicates_passed = 100 - duplicate_percentage\r\nduplicates_failed = duplicate_percentage\r\nduplicates_threshold_met = duplicates_failed <= alert_threshold\r\n\r\ndata_quality_report.append({\r\n    \"check_name\": \"Duplicate Records\",\r\n    \"passed\": duplicates_passed,\r\n    \"failed\": duplicates_failed,\r\n    \"threshold_percentage\": alert_threshold,\r\n    \"threshold_met\": duplicates_threshold_met\r\n})\r\n\r\n# Data Type Issues Check\r\ndata_types_passed = 100 - len(data_type_issues)\r\ndata_types_failed = len(data_type_issues)\r\ndata_types_threshold_met = data_types_failed <= alert_threshold\r\n\r\ndata_quality_report.append({\r\n    \"check_name\": \"Consistency Issues\",\r\n    \"passed\": data_types_passed,\r\n    \"failed\": data_types_failed,\r\n    \"threshold_percentage\": alert_threshold,\r\n    \"threshold_met\": data_types_threshold_met\r\n})\r\n\r\n# Final report\r\nreport = {\r\n    \"data_quality_report\": data_quality_report\r\n}\r\n\r\n# Step 7: Save report to S3 with a dynamic date\r\n\r\noutput_path_with_date = f\"{s3_output_path}/data_quality_report_{current_date}.json\"\r\n\r\nreport_json = json.dumps(report, indent=2)\r\ns3 = boto3.client(\"s3\")\r\nkey = \"/\".join(output_path_with_date.split('/')[3:])\r\ns3.put_object(Bucket=bucket_name, Key=key, Body=report_json)\r\n\r\nprint(f\"Report saved to S3 at: {output_path_with_date}\")\r\n\r\n# Step 8: Send alert if threshold is exceeded\r\nalert_message = \"\"\r\nif duplicates_failed > alert_threshold:\r\n    alert_message += f\"Duplicate records exceed threshold: {duplicates_failed:.2f}%\\n\"\r\n    \r\nfor col, percentage in missing_values_report.items():\r\n    if percentage > alert_threshold:\r\n        alert_message += f\"Missing values in column '{col}' exceed threshold: {percentage:.2f}%\\n\"\r\n\r\nif data_type_issues:\r\n    alert_message += f\"Data type mismatches detected: {data_type_issues}\\n\"\r\n\r\nif alert_message:\r\n    alert_message = f\"Data Quality Issues Detected:\\n{alert_message}\\nReport: {output_path_with_date}\"\r\n    sns_client = boto3.client(\"sns\")\r\n    sns_client.publish(\r\n        TopicArn=sns_topic_arn,\r\n        Message=alert_message,\r\n        Subject=\"Data Quality Alert\"\r\n    )\r\n    print(\"Alert sent via SNS\")\r\n\r\n# Step 9: Graceful exit\r\nif alert_message:\r\n    print(\"Warning: Data quality issues exceeded the threshold. Please check the report.\")\r\nelse:\r\n    print(\"Data quality checks completed successfully.\")\r\n"
}